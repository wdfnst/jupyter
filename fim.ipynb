{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 2017 机器学习第三次作业</center>\n",
    "| 姓名    |    分工   | 工作量 |\n",
    "|--------|-------------|------|\n",
    "| 张尉东  | 任务1,2      |33.33%|\n",
    "| 王海燕  | 任务1,2      |33.33%|\n",
    "| 张明悦  | 任务1,2      |33.33%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概念定义及数据预处理\n",
    "\n",
    "### (a.) 概念定义:    \n",
    "- **活跃作者**: 我们将2007至2017年分为三段```[2007, 2011), [2011, 2015), [2015, 2018)```, 并将活跃作者定义为三个阶段都出现的作者;\n",
    "- **团队**: 经常性在一起合作的学者称之为**团队**;\n",
    "- **频繁团队**: 至少一起发表过3篇以上论文的团队称之为**频繁团队**.\n",
    "\n",
    "### (b.) 数据结构定义:\n",
    "定义类型```class Article```, 将数据读入内存并存放在数组```articles```中, 代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, str):\n",
    "        self.authors = []\n",
    "        lines = str.split('\\n')\n",
    "        for line in lines:\n",
    "            if line is '' or line is None or line == \"\":\n",
    "                continue\n",
    "            key, value = line.split('\\t', 1)\n",
    "            if key == 'author':\n",
    "                self.authors.append(value)\n",
    "            elif key == 'title':\n",
    "                self.title = value\n",
    "            elif key == 'year':\n",
    "                self.year = int(value)\n",
    "            elif key == 'Conference':\n",
    "                self.conference = value\n",
    "    \n",
    "    def display(self):\n",
    "        str = ' '.join(self.authors) + ':' + self.title + ',' + self.conference\n",
    "        print(str)\n",
    "    \n",
    "    \n",
    "def parse_article(filename):\n",
    "    fin = open(filename)\n",
    "    content = fin.read()\n",
    "    blocks = content.split('#########')\n",
    "    articles = []\n",
    "    for block in blocks:\n",
    "        if block is '' or block is None or block == \"\":\n",
    "            continue\n",
    "        art = Article(block)\n",
    "        # art.display()\n",
    "        articles.append(art)\n",
    "    return articles\n",
    "        \n",
    "articles = parse_article('data/FilteredDBLP.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 活跃作者挖掘\n",
    "(**任务1.1**: 根据时间信息，判定哪些人依然活跃，哪些人不再活跃。)\n",
    "\n",
    "### (a.) ***活跃作者***挖掘及存储\n",
    "我们将挖掘出的活跃作者存储在一个```active_authors_years = dict()```结构体中, 其结构为:\n",
    "```python\n",
    "{'authorname':[year1, year2, ...]}\n",
    "```\n",
    "\n",
    "### (b.) 结果展示\n",
    "- ***活跃作者***: 如表2.1所以, 我们列出了部分活跃作者;\n",
    "- ***作者的活跃程度***: 如图2.1所示的散列图, 表示越活跃的作者(出现次数越多), 散列点越大;\n",
    "- ***连续活跃程度***: 如图2.2所示的活动轨迹图, 我们将活跃的作者(至少出现5次以上)依据其出现的年份连成一条活跃年份图."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  表2.1 活跃作者及活跃年份                   \n",
      "The number of active author: 1326\n",
      "Index    Author Name                         Years     \n",
      "-----    -------------                       --------- \n",
      "0        Fatih Murat Porikli                 {2008, 2014, 2015}\n",
      "1        Lars Schmidt-Thieme                 {2009, 2011, 2012, 2014, 2015}\n",
      "2        W. Bruce Croft                      {2016, 2017, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "3        Hankz Hankui Zhuo                   {2017, 2009, 2011, 2012, 2013, 2015}\n",
      "4        Kilian Q. Weinberger                {2016, 2017, 2008, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "5        Chang Huang                         {2016, 2009, 2010, 2011, 2012, 2015}\n",
      "6        Vernon Asuncion                     {2017, 2010, 2012, 2014}\n",
      "7        Yizhou Sun                          {2016, 2017, 2009, 2010, 2012, 2013, 2014, 2015}\n",
      "8        Arnold W. M. Smeulders              {2016, 2017, 2007, 2008, 2009, 2012, 2014, 2015}\n",
      "9        Derek Hoiem                         {2016, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2015}\n",
      "10       Francesco Bonchi                    {2016, 2008, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "11       Andrea Fusiello                     {2016, 2010, 2014, 2008}\n",
      "12       H. Sebastian Seung                  {2016, 2017, 2008, 2009, 2010, 2011, 2015}\n",
      "13       Jaime G. Carbonell                  {2016, 2017, 2007, 2008, 2009, 2010, 2011, 2013, 2014, 2015}\n",
      "14       Jussi Rintanen                      {2017, 2007, 2008, 2011, 2013, 2014, 2015}\n",
      "15       Edith Elkind                        {2016, 2017, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "16       Francesc Moreno-Noguer              {2017, 2009, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "17       Tomás Lozano-Pérez                  {2016, 2007, 2010, 2011, 2013, 2014, 2015}\n",
      "18       Philip S. Yu                        {2016, 2017, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "19       Piotr Dollár                        {2016, 2017, 2007, 2009, 2010, 2012, 2013, 2014, 2015}\n",
      "20       Weiming Hu                          {2016, 2017, 2008, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "21       Hamed Pirsiavash                    {2016, 2017, 2007, 2009, 2011, 2012, 2014, 2015}\n",
      "22       Wei Gao                             {2016, 2017, 2007, 2009, 2010, 2011, 2013, 2014, 2015}\n",
      "23       Madhav V. Marathe                   {2008, 2017, 2014, 2007}\n",
      "24       Dean P. Foster                      {2016, 2007, 2011, 2012, 2013, 2014, 2015}\n",
      "25       Charu C. Aggarwal                   {2017, 2007, 2009, 2010, 2011, 2013, 2014, 2015}\n",
      "26       Ping Tan                            {2016, 2017, 2007, 2008, 2009, 2010, 2012, 2013, 2014, 2015}\n",
      "27       Wouter M. Koolen                    {2016, 2017, 2008, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "28       Chen Zhang                          {2017, 2010, 2007, 2014, 2015}\n",
      "29       Wei Zeng                            {2016, 2017, 2008, 2011, 2013, 2014}\n",
      "30       Lirong Xia                          {2016, 2017, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015}\n",
      "31       Daniel Borrajo                      {2016, 2007, 2008, 2011, 2013, 2015}\n",
      "... ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1e57bcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "def find_active_author(articles):\n",
    "    # Statistics the number of author occurences and years\n",
    "    author_years = dict()\n",
    "    for article in articles:\n",
    "        for author in article.authors:\n",
    "            if author in author_years:\n",
    "                author_years[author].append(article.year)\n",
    "            else:\n",
    "                author_years[author] = [article.year]\n",
    "    active_author_years = dict()\n",
    "    for idx, kv in enumerate(author_years.items()):\n",
    "        key, value = kv\n",
    "        if len(value) > 3 and min(value) in range(2007, 2011) and max(value) in range(2015, 2018) and np.median(value) in range(2011, 2015):\n",
    "            active_author_years[key] = value\n",
    "    return active_author_years\n",
    "\n",
    "def display_active_authors(active_author_years, num):\n",
    "    print(\"{:<33} {:<33}\".format('', '表2.1 活跃作者及活跃年份'))\n",
    "    print('The number of active author:',len(active_author_years))\n",
    "    # formatly output the active auth, only output 50 items\n",
    "    print(\"{:<8} {:<35} {:<10}\".format('Index','Author Name','Years'))\n",
    "    print(\"{:<8} {:<35} {:<10}\".format('-----','-------------','---------'))\n",
    "    for idx, kv in enumerate(active_author_years.items()):\n",
    "        key, value = kv\n",
    "        print(\"{:<8} {:<35} {:<10}\".format(idx, key, str(set(value))))\n",
    "        if idx > num:\n",
    "            print('... ...')\n",
    "            break\n",
    "\n",
    "# 1. display the active authors and their active years\n",
    "active_author_years = find_active_author(articles)\n",
    "display_active_authors(active_author_years, 30)\n",
    "\n",
    "# 2. plot the ative level  \n",
    "fig1 = plt.figure(figsize=(20,10))\n",
    "def plot_active_level(active_author_years):\n",
    "    N = len(articles)\n",
    "    x = np.random.rand(N) * 3000\n",
    "    y = np.random.rand(N) * 3000\n",
    "    colors = np.random.rand(N)\n",
    "    area = np.pi * (15 * np.array([len(years)/100.0 for years in active_author_years.keys()]))**2  # 0 to 15 point radii\n",
    "\n",
    "    ax1 = fig1.add_subplot(121)\n",
    "    # ax1.xlim(0, 500)\n",
    "    ax1.set_xlim([0, 1000])\n",
    "    ax1.set_ylim([0, 1000])\n",
    "    ax1.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "    ax1.set_title('2.1 Active Level of Partial Author')\n",
    "    ax1.legend(numpoints=1, loc='upper left')\n",
    "\n",
    "# 3. plot the ative changes  \n",
    "def plot_active_changes(active_author_years, num):\n",
    "    ax2 = fig1.add_subplot(122)\n",
    "    ax2.set_title('2.2 Active changes')\n",
    "    ax2.set_xlim([2006, 2018])\n",
    "    years = [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "    ax2.set_xticks(years)\n",
    "    \n",
    "    for idx, active_author_years in enumerate(active_author_years.items()):\n",
    "        author, years = active_author_years\n",
    "        ax2.plot(years, np.ones(len(years)) * idx, 'o', years, np.ones(len(years)) * idx, '-')\n",
    "        if (idx > num):\n",
    "            break\n",
    "\n",
    "plot_active_level(active_author_years)\n",
    "plot_active_changes(active_author_years, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 频繁项挖掘\n",
    "(**任务1.2**: 经常性在一起合作的 学者，将之称为‘团队’, 根据研究者合作发表论文次数为根据 进行频繁模式挖掘，找出三个人以上的‘团队’.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a.) 找出支持度support>=5的所有学者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  表3.1 人数>=1, 合作次数>=5的团队           \n",
      "The number of active teams: 5985\n",
      "Index    Author Name                        \n",
      "-----    -------------                      \n",
      "0        Ming Yang                          \n",
      "1        Supun Samarasekera                 \n",
      "2        Fei-Fei Li, Vignesh Ramanathan     \n",
      "3        XuanLong Nguyen                    \n",
      "4        Kun Zhou                           \n",
      "5        Neil D. Lawrence                   \n",
      "6        Jörg H. Kappes                     \n",
      "7        Shireen Y. Elhabian                \n",
      "8        Subhransu Maji                     \n",
      "9        Yoram Bachrach, Yoad Lewenberg     \n",
      "10       Sang-Wook Kim                      \n",
      "... ...\n"
     ]
    }
   ],
   "source": [
    "from pymining import itemmining\n",
    "# transactions = (('a', 'b', 'c'), ('b'), ('a'), ('a', 'c', 'd'), ('b', 'c'), ('b', 'c'))\n",
    "# relim_input = itemmining.get_relim_input(transactions)\n",
    "# report = itemmining.relim(relim_input, min_support=2)\n",
    "def display_active_teams(active_teams, minimum_author, records):\n",
    "    print(\"{:<33} {:<33}\".format('', '表3.1 人数>=1, 合作次数>=5的团队'))\n",
    "    print('The number of active teams:',len(active_teams))\n",
    "    # formatly output the active auth, only output 50 items\n",
    "    print(\"{:<8} {:<35}\".format('Index','Author Name'))\n",
    "    print(\"{:<8} {:<35}\".format('-----','-------------','---------'))\n",
    "    counter = 0\n",
    "    for idx,authors in enumerate(active_teams):\n",
    "        if len(authors) >= minimum_author:  \n",
    "            print(\"{:<8} {:<35}\".format(counter, ', '.join(authors)))\n",
    "            counter += 1\n",
    "        if counter > records:\n",
    "            print('... ...')\n",
    "            break\n",
    "\n",
    "authors = [article.authors for article in articles]\n",
    "relim_input = itemmining.get_relim_input(authors)\n",
    "report = itemmining.relim(relim_input, min_support=5)\n",
    "display_active_teams(report, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b.) 找出合作者个数>=3的所有合作团队"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  表3.2 人数>=3, 合作次数>=5的团队           \n",
      "The number of active teams: 5985\n",
      "Index    Author Name                        \n",
      "-----    -------------                      \n",
      "0        Furu Wei, Ming Zhou 0001, Li Dong  \n",
      "1        Jun Xu, Jiafeng Guo, Xueqi Cheng, Yanyan Lan\n",
      "2        Horst Bischof, Amir Saffari, Christian Leistner\n",
      "3        Jiawei Han 0001, Chao Zhang, Quan Yuan\n",
      "4        Kate Saenko, Trevor Darrell, Marcus Rohrbach\n",
      "5        Xiaoqian Wang, Feiping Nie, Heng Huang\n",
      "6        Justin Zobel, Alistair Moffat, William Webber\n",
      "7        Michael R. Lyu, Haiqin Yang, Irwin King\n",
      "8        Kuldeep S. Meel, Supratik Chakraborty, Moshe Y. Vardi\n",
      "9        Min Zhang, Shaoping Ma, Yiqun Liu  \n",
      "10       Shiqiang Yang, Peng Cui, Fei Wang 0001\n",
      "... ...\n"
     ]
    }
   ],
   "source": [
    "display_active_teams(report, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 主题挖掘\n",
    "(**任务2.1**: 先定出主题词， 然后根据每个‘团队’发表的论文的情况，提炼出这个团队最常涉猎的主题。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a.) 根据Latent Dirichlet allocation(LDA)主题模型训练出所有的模型:\n",
    "我们将主题数设置为20, 经过LDA模型聚类, 我们得到了如表4.1所示的模型, 及其代表性的词."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "def train_ldamodel(articles):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # compile sample documents into a list\n",
    "    titles = [article.title for article in articles]\n",
    "    doc_set = []\n",
    "    for title in titles:\n",
    "        doc_set.append(title)\n",
    "\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)\n",
    "    return corpus, ldamodel\n",
    "\n",
    "corpus, ldamodel = train_ldamodel(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b.) 确定主题及主题词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  表4.1 20个主题及主题下的关键词               \n",
      "Topic-0              Topic-1                Topic-2              Topic-3              Topic-4               \n",
      "----------           ----------             ----------           ----------           ----------            \n",
      "Term         Prob     Term         Prob     Term         Prob     Term         Prob     Term         Prob    \n",
      "-----        -----    -----        -----    -----        -----    -----        -----    -----        -----   \n",
      "logic        0.056    select       0.063    variat       0.030    map          0.042    structur     0.093   \n",
      "program      0.045    camera       0.050    invari       0.028    data         0.037    shape        0.053   \n",
      "linear       0.037    featur       0.049    dataset      0.022    measur       0.032    learn        0.040   \n",
      "set          0.034    pose         0.043    level        0.022    consist      0.029    model        0.032   \n",
      "dynam        0.032    recommend    0.034    theoret      0.021    design       0.028    machin       0.031   \n",
      "reconstruct  0.032    awar         0.024    memori       0.019    driven       0.027    topic        0.025   \n",
      "descript     0.025    base         0.022    test         0.019    iter         0.025    sequenc      0.024   \n",
      "ontolog      0.023    video        0.021    resourc      0.019    studi        0.024    unsupervis   0.021   \n",
      "order        0.021    estim        0.021    paramet      0.017    tensor       0.023    self         0.020   \n",
      "express      0.019    use          0.019    revisit      0.016    k            0.022    stereo       0.018   \n",
      "\n",
      "\n",
      "Topic-5              Topic-6                Topic-7              Topic-8              Topic-9               \n",
      "----------           ----------             ----------           ----------           ----------            \n",
      "Term         Prob     Term         Prob     Term         Prob     Term         Prob     Term         Prob    \n",
      "-----        -----    -----        -----    -----        -----    -----        -----    -----        -----   \n",
      "human        0.052    visual       0.077    inform       0.070    analysi      0.112    optim        0.055   \n",
      "motion       0.048    singl        0.045    base         0.044    discrimin    0.031    algorithm    0.046   \n",
      "robot        0.030    relat        0.042    knowledg     0.038    intellig     0.025    larg         0.041   \n",
      "use          0.026    interact     0.033    web          0.033    confer       0.023    scale        0.041   \n",
      "decis        0.024    estim        0.032    retriev      0.030    enhanc       0.022    fast         0.032   \n",
      "system       0.022    depth        0.032    event        0.026    correl       0.021    rank         0.031   \n",
      "pattern      0.021    partial      0.031    space        0.023    statist      0.020    method       0.026   \n",
      "behavior     0.021    discoveri    0.026    exploit      0.020    workshop     0.019    factor       0.025   \n",
      "model        0.020    observ       0.021    argument     0.019    latent       0.019    matrix       0.024   \n",
      "social       0.019    vote         0.020    locat        0.016    ijcai        0.019    probabilist  0.023   \n"
     ]
    }
   ],
   "source": [
    "# ldamodel = train_ldamodel(articles)\n",
    "ldamodel.print_topics()\n",
    "ldamodel.show_topic(10)\n",
    "print(\"{:<33} {:<33}\".format('', '表4.1 20个主题及主题下的关键词'))\n",
    "for i in range(2):\n",
    "    topic_terms1 = ldamodel.show_topic(i * 5)\n",
    "    topic_terms2 = ldamodel.show_topic(i * 5 + 1)\n",
    "    topic_terms3 = ldamodel.show_topic(i * 5 + 2)\n",
    "    topic_terms4 = ldamodel.show_topic(i * 5 + 3)\n",
    "    topic_terms5 = ldamodel.show_topic(i * 5 + 4)\n",
    "    #print(topic_terms1)\n",
    "    print(\"{:<20} {:<22} {:<20} {:<20} {:<22}\".format('Topic-' + str(i * 5), 'Topic-' + str(i * 5 + 1), 'Topic-'+ str(i * 5 + 2), 'Topic-'+ str(i * 5 + 3), 'Topic-'+ str(i * 5 + 4)))\n",
    "    print(\"{:<20} {:<22} {:<20} {:<20} {:<22}\".format('----------','----------','----------','----------','----------'))\n",
    "    print(\"{:<12} {:<8} {:<12} {:<8} {:<12} {:<8} {:<12} {:<8} {:<12} {:<8}\".format('Term','Prob','Term','Prob','Term','Prob','Term','Prob','Term','Prob',))\n",
    "    print(\"{:<12} {:<8} {:<12} {:<8} {:<12} {:<8} {:<12} {:<8} {:<12} {:<8}\".format('-----','-----','-----','-----','-----','-----','-----','-----','-----','-----',))\n",
    "    for term_prob1, term_prob2, term_prob3, term_prob4, term_prob5 in zip(topic_terms1, topic_terms2, topic_terms3, topic_terms4, topic_terms5):\n",
    "        # term1, prob1 = term_prob1\n",
    "        print(\"{:<12} {:<8} {:<12} {:<8} {:<12} {:<8} {:<12} {:<8} {:<12} {:<8}\".format(term_prob1[0], \"{0:.3f}\".format(term_prob1[1]), term_prob2[0], \"{0:.3f}\".format(term_prob2[1]), term_prob3[0], \"{0:.3f}\".format(term_prob3[1]), term_prob4[0], \"{0:.3f}\".format(term_prob4[1]), term_prob5[0], \"{0:.3f}\".format(term_prob5[1])))\n",
    "    if i== 0:\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c.) 确定团队经常涉猎的主题\n",
    "使用映射 `作者<--->标题<--->主题` 将作者经常所涉猎的主题进行关联:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          表4.2 研究团队经常涉猎的主题及其主题下的关键词          \n",
      "Team memebers                                           Topic IDs       Topic Terms                        \n",
      "--------------------                                    ---------       -----------------------            \n",
      "Miroslaw Truszczynski, Stefan Woltran                   291118          recognit, face, languag            \n",
      "Guibing Guo, Jie Zhang, Neil Yorke-Smith                38141517        network, deep, neural              \n",
      "Xinwang Liu, Yong Dou, Jianping Yin, Lei Wang...        10141718        learn, decis, tree                 \n",
      "Mickey Brautbar, Michael J. Kearns, Umar Syed           4691213161718   logic, cluster, base               \n",
      "Shirin Sohrabi, Anton V. Riabov, Octavian Udrea         261618          logic, cluster, base               \n",
      "Maximilian Nickel, Lorenzo Rosasco, Tomaso A....        681318          featur, select, embed              \n",
      "Kyle Lund, Sam Dietrich, Scott Chow, James C....        26916           tempor, data, detect               \n",
      "Leandro Soriano Marcolino, Boian Kolev, Samori...       6111417         tempor, data, detect               \n",
      "Russell Bent, Alan Berscheid, G. Loren Toole            231516          plan, time, motion                 \n",
      "Vibhav Gogate, Rina Dechter                             21018           logic, cluster, base               \n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "topics = ldamodel.get_topics()\n",
    "# print(ldamodel.show_topic(19, topn=3))\n",
    "doc2topics = ldamodel.get_document_topics(corpus)\n",
    "# print(len(doc2topics))\n",
    "# print(doc2topics[0])\n",
    "# print(doc2topics[1])\n",
    "# len(doc2topics)\n",
    "\n",
    "print(\"{:<25} {:<35}\".format('', '表4.2 研究团队经常涉猎的主题及其主题下的关键词'))\n",
    "print(\"{:<55} {:<15} {:<35}\".format('Team memebers', 'Topic IDs', 'Topic Terms'))\n",
    "print(\"{:<55} {:<15} {:<35}\".format('--------------------','---------', '-----------------------'))\n",
    "for idx, article in enumerate(articles[:10]):\n",
    "    max_prob_topic = 0\n",
    "    max_prob = 0.0\n",
    "    # Find the topic_id with maximum probability\n",
    "    topic_id_str = \"\"\n",
    "    for topic_id, prob in doc2topics[idx]:\n",
    "        topic_id_str += str(topic_id)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_prob_topic = topic_id\n",
    "    topic_terms = [topic_prob[0] for topic_prob in ldamodel.show_topic(max_prob_topic, topn=3)]\n",
    "    print(\"{:<55} {:<15} {:<35}\".format(textwrap.shorten(', '.join(article.authors), width=50, placeholder=\"...\"), topic_id_str, ', '.join(topic_terms)))\n",
    "    #str = ', '.join(article.authors) + \"--->\" + ', '.join(topic_terms)\n",
    "    #print(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 团队和主题多是会随着时间而动态变化\n",
    "(**任务2.2**: 根据自己所定的时间段(五年，三年，两年或是一年)描述团队的构成状况以及其研究主题的变化情况。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
