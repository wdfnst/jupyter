{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据进行预处理\n",
    "将数据读入内存, 并封装在Article类中."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, str):\n",
    "        self.authors = []\n",
    "        lines = str.split('\\n')\n",
    "        for line in lines:\n",
    "            if line is '' or line is None or line == \"\":\n",
    "                continue\n",
    "            key, value = line.split('\\t', 1)\n",
    "            if key == 'author':\n",
    "                self.authors.append(value)\n",
    "            elif key == 'title':\n",
    "                self.title = value\n",
    "            elif key == 'Conference':\n",
    "                self.conference = value\n",
    "    \n",
    "    def display(self):\n",
    "        str = ' '.join(self.authors) + ':' + self.title + ',' + self.conference\n",
    "        print(str)\n",
    "    \n",
    "    \n",
    "def parse_article(filename):\n",
    "    fin = open(filename)\n",
    "    content = fin.read()\n",
    "    blocks = content.split('#########')\n",
    "    articles = []\n",
    "    for block in blocks:\n",
    "        if block is '' or block is None or block == \"\":\n",
    "            continue\n",
    "        art = Article(block)\n",
    "        # art.display()\n",
    "        articles.append(art)\n",
    "    return articles\n",
    "        \n",
    "articles = parse_article('data/FilteredDBLP.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 活跃作者挖掘\n",
    "根据时间信息，看看哪些人依然活跃，哪些 人不再活跃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 频繁项挖掘\n",
    "经常性在一起合作的 学者，将之称为‘团队’, 根据研究者合作发表论文次数为根据 进行频繁模式挖掘，找出三个人以上的‘团队’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 找出支持度support>=5的所有学者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymining import itemmining\n",
    "# transactions = (('a', 'b', 'c'), ('b'), ('a'), ('a', 'c', 'd'), ('b', 'c'), ('b', 'c'))\n",
    "# relim_input = itemmining.get_relim_input(transactions)\n",
    "# report = itemmining.relim(relim_input, min_support=2)\n",
    "\n",
    "authors = [article.authors for article in articles]\n",
    "relim_input = itemmining.get_relim_input(authors)\n",
    "report = itemmining.relim(relim_input, min_support=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 找出合作者个数大于3的所有合作团队"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for r in report:\n",
    "    if len(r) >= 3:\n",
    "        counter += 1\n",
    "       # print(r)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 主题挖掘\n",
    "先定出主题词， 然后根据每个‘团队’发表的论文的情况，提炼出这个团队最常涉猎 的主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据Latent Dirichlet allocation(LDA)主题模型训练出所有的模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# compile sample documents into a list\n",
    "titles = [article.title for article in articles]\n",
    "doc_set = []\n",
    "for title in titles:\n",
    "    doc_set.append(title)\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用***作者<--->标题<--->主题***这个映射将作者经常所涉猎的主题进行关联:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miroslaw Truszczynski, Stefan Woltran--->logic, program, probabilist\n",
      "Guibing Guo, Jie Zhang, Neil Yorke-Smith--->inform, improv, retriev\n",
      "Xinwang Liu, Yong Dou, Jianping Yin, Lei Wang 0001, En Zhu--->graph, match, cluster\n",
      "Mickey Brautbar, Michael J. Kearns, Umar Syed--->factor, matrix, decis\n",
      "Shirin Sohrabi, Anton V. Riabov, Octavian Udrea--->effici, high, search\n",
      "Maximilian Nickel, Lorenzo Rosasco, Tomaso A. Poggio--->graph, match, cluster\n",
      "Kyle Lund, Sam Dietrich, Scott Chow, James C. Boerkoel--->search, tempor, visual\n",
      "Leandro Soriano Marcolino, Boian Kolev, Samori Price, Sreerag Palangat Veetil, David Jason Gerber, Josef Musil, Milind Tambe--->optim, larg, scale\n",
      "Russell Bent, Alan Berscheid, G. Loren Toole--->social, person, joint\n",
      "Vibhav Gogate, Rina Dechter--->robust, estim, hierarch\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.get_topics()\n",
    "# print(ldamodel.show_topic(19, topn=3))\n",
    "doc2topics = ldamodel.get_document_topics(corpus)\n",
    "# print(len(doc2topics))\n",
    "# print(doc2topics[0])\n",
    "# print(doc2topics[1])\n",
    "# len(doc2topics)\n",
    "\n",
    "for idx, article in enumerate(articles[:10]):\n",
    "    max_pro_topic = 0\n",
    "    max_pro = 0.0\n",
    "    # Find the topic_id with maximum probability\n",
    "    for topic_id, pro in doc2topics[idx]:\n",
    "        if pro > max_pro:\n",
    "            max_pro = pro\n",
    "            max_pro_topic = topic_id\n",
    "    # print(max_pro_topic)\n",
    "    topic_terms = [topic_pro[0] for topic_pro in ldamodel.show_topic(max_pro_topic, topn=3)]\n",
    "    str = ', '.join(article.authors) + \"--->\" + ', '.join(topic_terms)\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 团队和主题多是会随着时间而动态变化\n",
    "根据自己所定的时 间段(五年，三年，两年或是一年)描述团队的构成状况以及其 研究主题的变化情况。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
